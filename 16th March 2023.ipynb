{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b7a25caa",
   "metadata": {},
   "source": [
    "Ans 1: Overfitting refers to a situation in machine learning where a model learns to fit the training data too closely, resulting in poor performance on new, unseen data. On the other hand, underfitting occurs when a model is too simple and fails to capture the underlying patterns in the data, resulting in poor performance on both the training and test data.\n",
    "\n",
    "The consequences of overfitting are that the model will perform well on the training data but poorly on new data. Underfitting, on the other hand, results in poor performance on both the training and test data.\n",
    "\n",
    "To mitigate overfitting, we can use techniques such as regularization, early stopping, and cross-validation. Underfitting can be mitigated by increasing the complexity of the model, adding more features, or using more advanced algorithms.\n",
    "\n",
    "Ans 2: To reduce overfitting, we can use techniques such as regularization, early stopping, and cross-validation. Regularization involves adding a penalty term to the loss function to encourage the model to learn simpler patterns. Early stopping involves stopping the training process before the model overfits the training data. Cross-validation involves splitting the data into multiple training and validation sets to evaluate the model's performance.\n",
    "\n",
    "Ans 3:Bias is the difference between the expected predictions of a model and the true values of the data. A high bias model is one that is too simple and makes overly simplified assumptions about the relationship between the input features and output variable. Such models are not able to capture the underlying patterns in the data, resulting in underfitting.\n",
    "\n",
    "Variance, on the other hand, is the variability of the model's predictions for different training datasets. A high variance model is one that is too complex and overfits the training data, resulting in poor generalization performance on new, unseen data.\n",
    "\n",
    "The bias-variance tradeoff states that as the complexity of a model increases, its variance increases, while its bias decreases. Conversely, as the complexity of a model decreases, its bias increases, while its variance decreases.\n",
    "\n",
    "Ans 4: Common methods for detecting overfitting and underfitting in machine learning models include evaluating the model's performance on the training and test data, visualizing the training and validation loss over time, and using cross-validation to evaluate the model's performance on multiple subsets of the data. To determine whether a model is overfitting or underfitting, we can compare its performance on the training and test data. If the model performs well on the training data but poorly on the test data, it is likely overfitting. If the model performs poorly on both the training and test data, it is likely underfitting.\n",
    "\n",
    "Ans 5: Bias and variance are both sources of error in machine learning models. High bias models are too simple and fail to capture the underlying patterns in the data, resulting in underfitting. High variance models, on the other hand, are too complex and overfit the training data, resulting in poor performance on new data. Examples of high bias models include linear regression models and models with few features. Examples of high variance models include decision trees and deep neural networks.\n",
    "\n",
    "Ans 6: Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. The penalty term discourages the model from learning complex patterns that may only be present in the training data and may not generalize well to new, unseen data.\n",
    "\n",
    "Some common regularization techniques include:\n",
    "\n",
    "L1 Regularization: L1 regularization adds a penalty term proportional to the absolute value of the weights to the loss function. This encourages the model to learn sparse weights, effectively reducing the number of features used by the model.\n",
    "\n",
    "L2 Regularization: L2 regularization adds a penalty term proportional to the square of the weights to the loss function. This encourages the model to learn smaller weights, effectively reducing the impact of individual features.\n",
    "\n",
    "Dropout: Dropout is a technique where random neurons are dropped out of the network during training. This helps prevent overfitting by forcing the network to learn redundant representations of the data.\n",
    "\n",
    "Regularization techniques work by adding a penalty term to the loss function, effectively controlling the complexity of the model. By reducing the model's capacity to learn complex patterns, regularization helps prevent overfitting by encouraging the model to learn more generalizable patterns that are applicable to new, unseen data.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
